"""
Sample problems on which to test algorithms.

"""
import numpy

import base
from pyll import scope


class Base(base.Bandit):
    def __init__(self, template):
        self.rng = numpy.random.RandomState(55)
        base.Bandit.__init__(self, template)

    def dryrun_config(self):
        return self.template.render_sample(self.rng)

    def evaluate(self, config, ctrl):
        return dict(
                loss = -self.score(config),
                status = 'ok')


class Quadratic1(Base):
    """
    About the simplest problem you could ask for:
    optimize a one-variable quadratic function.
    """

    loss_target = 0

    def __init__(self):
        Base.__init__(self, dict(x=scope.uniform(-5, 5)))

    def score(self, config):
        return -(config['x'] - 3)**2


class Q1Lognormal(Base):
    """
    About the simplest problem you could ask for:
    optimize a one-variable quadratic function.
    """

    loss_target = 0

    def __init__(self):
        Base.__init__(self, dict(x=scope.lognormal(0, 2)))

    def score(self, config):
        return max(-(config['x'] - 3) ** 2, -100)


class TwoArms(Base):
    """
    Each arm yields a reward from a different Gaussian.

    How long does it take the algorithm to identify the best arm?
    """

    loss_target = -2

    def __init__(self):
        Base.__init__(self, dict(x=scope.one_of(0, 1)))

    def score(self, config):
        arms = 2
        reward_mus = [1] + [0] * (arms - 1)
        reward_sigmas = [1] * arms
        return self.rng.normal(size=(),
                loc=reward_mus[config['x']],
                scale=reward_sigmas[config['x']])

    @classmethod
    def loss_variance(cls, result, config):
        return 1.0


class Distractor(Base):
    """
    This is a nasty function: it has a max in a spike near -10, and a long
    asymptote that is easy to find, but guides hill-climbing approaches away
    from the true max.
    """

    loss_target = -2

    def __init__(self, sigma=10):
        """
        The second peak is at x=-10.
        The prior mean is 0.
        """
        Base.__init__(self, dict(x=scope.normal(0, sigma)))

    def score(self, config):
        f1 = 1.0 / (1.0 + numpy.exp(-config['x']))    # climbs rightward from 0.0 to 1.0
        f2 = 2 * numpy.exp(-(config['x'] + 10) ** 2)  # bump with height 2 at (x=-10)
        return f1 + f2

    def loss_variance(self, result, config=None):
        return 0.0


class GaussWave(Base):
    """
    Essentially, this is a high-frequency sinusoidal function plus a broad quadratic.
    One variable controls the position along the curve.
    The binary variable determines whether the sinusoidal is shifted by pi.

    So there are actually two maxima in this problem, it's just one is more
    probable.  The tricky thing here is dealing with the fact that there are two
    variables and one is discrete.

    """

    loss_target = -1

    def __init__(self):
        Base.__init__(self, dict(
            curve=scope.one_of(0, 1),
            x=scope.uniform(-20, 20)))

    def score(self, config):
        if config['curve']:
            x = config['x']
        else:
            x = config['x'] + numpy.pi
        f1 = numpy.sin(x)                # climbs rightward from 0.0 to 1.0
        f2 = 2 * numpy.exp(-(x/5.0)**2)  # bump with height 2 at (x=-10)
        return f1 + f2

    def loss_variance(self, result, config=None):
        return 0.0


class GaussWave2(Base):
    """
    Variant of the GaussWave problem in which noise is added to the score
    function, and there is an option to either have no sinusoidal variation, or
    a negative cosine with variable amplitude.

    Immediate local max is to sample x from spec and turn off the neg cos.
    Better solution is to move x a bit to the side, turn on the neg cos and turn
    up the amp to 1.
    """

    loss_target = -3

    def __init__(self):
        Base.__init__(self, dict(
            x=scope.uniform(-20, 20),
            hf=scope.one_of(
                dict(kind='raw'),
                dict(kind='negcos', amp=scope.uniform(0, 1)))))

    def score(self, config):
        r = self.rng.randn() * .1
        x = config['x']
        r += 2 * numpy.exp(-(x/5.0)**2) # up to 2
        if config['hf']['kind'] == 'negcos':
            r += numpy.sin(x) * config['hf']['amp']

        return r

    def loss_variance(self, result, config=None):
        return 0.01

